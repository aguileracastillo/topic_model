---
title: "Finding a needle in a haystack: A Structural Topic Model of the DGRL v17.5"
author: "Andres Aguilera Castillo"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  pdf_document: default
  html_document: default
bibliography: references.bib
---

```{r data, include = FALSE}
library(tidyverse)
library(tidytext)
library(readr)
library(dplyr)
library(here)
library(naniar)
library(knitr)
library(magrittr)
library(ggplot2)
# Import DGRL version 17.5 (2021/12/15) 16531 References
# https://faculty.washington.edu/jscholl/dgrl/index.php 
# Master RIS file 
mf_ris <- read.csv(here("data", "DGRL_Lit_Master_v17_ris.csv"))
mf_ris <- as_tibble(mf_ris)
mf_ris <- mf_ris %>%
  replace_with_na_all(condition = ~.x %in% common_na_strings)

## Rename Variables of Interest
mf_ris <- mf_ris %>% rename(type = `Item.Type`, year = `Publication.Year`, author = `Author`, doc_title = `Title`, pub_title = `Publication.Title`, abstract = `Abstract.Note`)

# Corpus after initial cleaning
to_corpus <- read.csv(here("data", "to_corpus.csv"))
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Abstract

Digital Government is a growing and vibrant multidisciplinary field of research, the fast increase in research output has challenged researchers to explore and use novel computational ways and methods to perform evidence synthesis on the extant literature and be able to map a scientific discipline, explore the thematic evolution over time and identify potential avenues for further research. The exploration of the linkages between digitalization and the organization of work remains relatively unexplored in a public sector context. Topic modeling has emerged as a powerful computational technique that contributes to the examination of large amounts of text data. This chapter aims to perform a 'smart literature review' on a subset of the Digital Government Reference Library (DGRL) version 17.5, by using text as data approach and a topic modeling technique known as Latent Dirichlet Allocation (LDA). To our best knowledge, this is the first attempt to use unsupervised machine learning techniques with this data set. This effort may contribute to creating a map of the field, identify the evolving themes in the literature and help to identify promising areas of research.

## Introduction

Recent trends in global scientific output demonstrate a rapid and sustained increase in the production of vast amounts of unstructured data in the form of digitized text. This bounty in research content is challenging researchers to explore and pursue novel methodological approaches and techniques to examine massive volumes of scientific publications. Essentially, the ever-growing amounts of bibliographic information available in almost any field of research exceed human capacity making it necessary to explore computational-assisted approaches for research synthesis. Studies in the history of science have identified a relatively sustained growth pattern in scientific publications over time, this exponential growth rate means a doubling in scientific output every 17 years approximately [@bornmann2021]. This level of growth might be attributed to the increased resources dedicated to the global scientific endeavor and consequently the communication of science via publications. However, it may also be due to what has been dubbed "salami sliced publishing" or the multiple publications of a single research study [@bornmann2007a; @bornmann2015].

Digital Government Research (DGR) as a multidisciplinary research field is experiencing rapid growth in its research output. Contributions to this research domain come from established disciplines such as information science, computer science, organization science, sociology, public administration, and political science [@scholl2021]. The diversity in scope and methods of these disciplines converge in the field of Digital Government enriching it, but at the same time, raising questions as regards the lack of native theoretical developments, thus relying upon frameworks, theories, and conceptualizations from related disciplines [@bannister2015].

The advent of computerization and digitalization has had broad impacts in most aspects of contemporary life, including scientific research. Digitalization has influenced how research is designed and conducted, allowing for the creation and increased availability of ever-growing data sets that require powerful computational methods and enhanced tools to handle abundant information [@meyer2015]. Case in point is unsupervised machine learning techniques for text analysis, this research technology can be used in a wide range of disciplines to examine databases, repositories and corpora thus expanding the methodological repertoire of scholars and opening an opportunity to explore large troves of data.

Research synthesis is part of the literature review process in which the extant scientific knowledge in each academic field is examined to help scholars understand the conceptual structure, themes, and debates to identify trends in the literature and potential areas for further research. This crucial task is labor-intensive, time-consuming, and restricted to a limited number of documents if conducted by traditional "manual" methods [@antons2020a; @asmussen2019]. Still, computer-assisted text analysis does not substitute human intervention, instead it "augments our reading ability" [@grimmer2022], human judgement is deemed necessary for the evaluation and validation of the outcome of these models [@barberá2021].

This chapter explores the contents of The Digital Government Research Library in its version 17.5 (Scholl, 2021a), using a text as data approach and an unsupervised machine learning technique known as topic modeling. Probabilistic topic modeling is a method that extracts topics from a collection of text. According to the seminal work by @blei2003, Latent Dirichlet Allocation (LDA) applied to a corpus generates a probabilistic model in which documents are represented as the mixtures of latent topics, and topics are characterized by a distribution of words. LDA is considered the state-of-the-art, simplest and most used method to perform topic modeling [@asmussen2019a].

According to the DGRL website, version 17.5 of the data set contains 16531 references related to the Digital Government Research (DGR) domain, the most prevalent types of documents are conference papers (33.2%) and journal articles (50%). The inclusion criteria of the DGRL are: to have passed academic peer review, to be published in an academic journal, to be published in English language [@scholl2021].

```{r types of documents, echo=FALSE}
by_type <- mf_ris %>% count(`type`) %>% arrange(desc(n))
kable(by_type, col.names = c("Document Type", "Number of Documents"), align = "cc", caption = "Table 1. Contents of DGRL v17.5 without pre-processing")
```

Previous explorations of this reference library using bibliometric and scientometric approaches have revealed the thematic evolution [@alcaide-muñoz2017], and identified the most influential journals, conferences and leading scholars in the field [@scholl2021a]. This data set has been used as well for conducting a systematic review on the impacts of e-Government using a public value perspective [@maclean2021a]. According to [@webster2002], accumulating a "complete census" of the relevant literature and following a concept-centric framework are crucial in a literature review. Topic modeling enables the use of larger bibliographic data sets, and the extraction of relevant concepts from sizable corpora in a scalable way. To our best knowledge this is the first attempt to run a topic model for a corpus in the field of Digital Government Research.

The impact of modern technologies (i.e. automation, digitalization, robotization) on labor has been on the research agenda of diverse disciplines and academic fields but explored mostly for a private sector context in advanced democracies. The public sector is not only a major employer in a variety of activities but also it is considered one of the largest adopters of information and communication technologies (ICT). The public sector commands a large, diverse, and in general, highly educated workforce. Recent technological developments have allowed the deployment of digital technologies, digital platforms, and digital infrastructures by both private corporations and public organizations with deep implications for the organization of work [@nambisan2019].

The reported impact of digitalization on the organization work is diverse [@barley2001], it may automate work, create or eliminate jobs, deskill or reskill workers but also, little or negligible impact whatsoever. Digital government as a research field is in a phase of consolidation, allowing for the exploration of promising subfields for further inquiry. Despite the momentum in digital government research, one aspect that, to the best of our knowledge remains underexplored, is the effects of technological change on the public sector workforce [@plesner2018].

Digitalization, in general, allows a more specific division of labor into the smallest possible tasks [@cherry2015], and in a public service context, opens more opportunities for the implementation of self-service solutions and facilitating scenarios for the co-production of public services [@scupola2021], turning each citizen and user into "his or her own administrator, caseworker and bureaucrat" [@schou2018], and possibly generating administrative burden of citizens [@madsen2021].

Digital technologies and the novel design of public services may facilitate a more intricate division of labor into smaller components (tasks), reconfiguring the workflow of public services, fostering new ways for multi-actor co-production, and probably generating tasks, if not job redundancies, but also creating new occupations to cope with an increasingly digitalized public sector. Therefore, it is deemed pertinent and timely to extend the scholarly exploration of the effects of technological change in the organization of work in public organizations and the potential consequences for the public sector workforce.

RQ1: What does topic modeling techniques applied to DGRL v17.5 reveal about the conceptual, intellectual and thematic evolution this academic field? --\> Run LDA

RQ2: What structural changes can be interpreted from the topic model? --\> (Covariates)

RQ3: What does the extant literature (corpus) on Digital Government reveal on the linkage between digitalization and the organization of work?

Process

## Literature Review // Conceptual framework

Quantitative research synthesis techniques like bibliometrics and computer-assisted text mining allow the analysis of a larger quantity of documents and may contribute to advancing the "research fronts" in interdisciplinary fields such as Digital Government [@tanskanen2017]. Computational tools and techniques developed originally in the computer science field have been repurposed in diverse disciplines but also have enabled social scientists to exploit Natural Language Processing (NLP) applications for classification tasks of large scientific corpora. Topic modeling techniques, a subset of machine learning and NLP allow for the automatic classification of vast amounts of text and have been used for the analysis of bibliographic content in diverse fields of research and academic disciplines including statistics [@debattisti2015], economics [@ambrosino2018], cliometrics [@wehrheim2019], innovation research [@antons2020b; @antons2017], and management [@hannigan2019].

On occasions the scope of analysis can be very large, [@ambrosino2018] studied the evolution in the thematic structure of the economics discipline by applying LDA to the full texts of articles published in 188 journals in the JSTOR database from 1845 to 2013 (n= 250846). Other implementations of these techniques [@antons2016], have explored the full text corpus of a single top ranking journal in innovation research over a three decade span (n=1008), alternative uses of this technique have considered the titles of dissertations in economics and chemistry in East and West Germany before and after the German reunification [@rehs2020].

As advised by [@barberá2021], there are "consequential decisions" in the methodological choices of automated text classification and the fact that human validation is a key component of text as data methods. The selection of a corpus in itself is deemed a crucial decision that can be prone to four types of bias: resource bias, incentive bias, medium bias and retrieval bias, these selection biases are well acknowledged in the text as data literature [@grimmer2022]. It may be probable that the DGRL v17.5 has omitted important research that is not included in this collection. All decisions concerning text as data methodologies are "consequential", our aim is to make our workflow reproducible by documenting all the choices in the scripts associated with this document.

In addition, the use of text based techniques and topic models have gained traction among scholars exploring the nexus between novel technologies and labor markets. Among these novel approaches are [@montobbio2022] that explore robots and labor-saving technologies, and [@kogan2019] that analyze patent contents to estimate technological change and labor displacement. In our opinion, this methodological innovation can be repurposed to explore the linkages between digitalization and organization of work in a public sector context.

LDA models are becoming widely used in social science, however these techniques are not infallible and require rigorous validation and human interpretability [@maier2018], if not, it may be as factual as "reading tea leaves" as eloquently put it by [@chang2009]. For a robust analysis it is advised to take an iterative approach for build, compute, critique, and rebuild topic models [@blei2014].

The implementation of LDA to bibliographic data has a relatively agreed-upon workflow. The script for the initial data cleaning and wrangling, including the R functions and packages used, is available for revision, clarity, and replicability. A second script describes the phase of pre-processing related to preparing the unstructured text data into a format that is usable for analysis. Steps like tokenization, removal of stopwords, symbols, and special characters, and conversion to lowercase, are part of this phase. The pre-processing was conducted in R statistical software using the functions of the quanteda R package [@benoit2018].

LDA is an unsupervised machine learning method which means the relationship between words and topics is ignored prior to the execution of the model. Thus is deemed good practice to split the data between a training set and a test set. Our approach was to train with 75% of the corpus, leaving the remaining proportion for testing purposes. In addition, LDA topic modeling typically means that the topic structure is unknown and the number of topics (k) should be selected by the researcher. In general, a low number of topics is used for an overview, instead, a higher number of topics is used for more granular analysis of the corpus [@asmussen2019a].

Model optimization measures: predictive likelihood \* perplexity \* coherence \* exclusivity \* cutoff criteria loadings Antons et al 2016.

Maier

Evidence synthesis --\>

What is digital government?

Collect names (Buffat et al)

STM

reproducibility

human interpretable

Insert list of top journals.

## Methods and Data

The Digital Government Research Library version 17.5 is a large curated repository of publications contributing to the field of Digital Government Research (DGR), it contains more than 16500 references among its records. The Library can be downloaded from the website [DGRL](https://faculty.washington.edu/jscholl/dgrl/index.php). The download package contains three types of bibliographic files BibTeX, RIS, and ENL (EndNote). In its raw and unprocessed form, the data has a large proportion of missing values, mostly clustered in metadata not considered relevant for the analysis. By exploring the BiBTeX, RIS and ENL files, we noticed that the data sets had a large amount of missing data and that some information was available in a file type but not other. For this exercise, the following variables have been deemed of interest for the analysis: type of reference (conference paper or journal article), year of publication, author, document title, publication title and the presence of an abstract. Text is a type of unstructured data that requires meticulous processing before using it. For replicability purposes, the script for data wrangling, cleaning and overall processing of the topic model is available in the scripts section of the [GitHub](https://github.com/aguileracastillo/topic_model) repository for this project.

The corpus used for this analysis is a subset of the journal articles in the version 17.5 of the Digital Government Reference Library. As argued by @grimmer2022a, texts are "expensive to produce, gather and collate", the contents of previous versions of this data set have been used as primary or secondary source of data exploring the Digital Government field. The approach of this chapter is not as ambitious as Ambrosino *et. al.*, nor focalized on a single journal publication as Antons *et. al.*, instead the aim is to analyze the abstracts of 6682 journal articles in the version 17.5 of the DGRL via the application of a topic model.

After the initial data wrangling, the relevant data for 6682 journal articles or approximately 80.7% of the total number of articles in the DGRL v17.5 is further processed to create a corpus, the initial step towards a topic model. A visualization in the publication trend demonstrate an incipient increase in number of journal articles after year 2000 and a steep increase in the beginning of the 2010 decade to present.

```{r articles by year, echo = FALSE, include = TRUE}
## Insert from 02_topic_model script
dgrl175_tm <- to_corpus %>% filter(type.x == "journalArticle")
dgrl175_tm %>% group_by(year.x) %>% count(sort = TRUE) %>%
  ggplot(aes(year.x, n)) +
  geom_line()+
  xlab ("Year") +
  ylab ("Number of Documents")
```

The workflow for topic modeling includes text pre-processing, meaning further data cleaning and data transformation. This means that before creating a corpus object with the available information from the DGRL v17.5, the text should be prepared before running the initial explorations. Our main unit of analysis is the abstract of the journal articles contained in the processed data set of the DGRL v17.5. Text as a type of unstructured data can be structured for processing using the bag of words approach or the splitting of abstracts into separate word units or terms and every occurrence of a term is defined as a token. The creation of a bag of words is known as tokenizing. The bag of words approach deliberately ignores the syntax or structure of the text, additional treatment of text include the elimination of punctuation, transform each word to lowercase and in some cases stemming which is a way to reduce a word to its stem or root in order to reduce the sparsity of the Document Frequency Matrix.

Stop words are context specific

Split into train and test data set Define K perplexity Asmussen (more or less topics) granularity.

LDA? Unsupervised learning... latent topics.

As suggested by @webster2002a, a complete review covers the relevant literature and it is not limited by a single research methodology, set of journals or geographic region. In this exercise, the top 10 publication titles (journal name) in the corpus represent almost a third of the documents in the sample. By making quick title search in the Scimago Journal Rank website, it can be established that all names on the table are listed in this database.

```{r top 10 journals in the sample, echo = FALSE, include = TRUE}
top10 <- dgrl175_tm %>% count(pub_title.x) %>% arrange(desc(n)) %>% head(10)
kable(top10, col.names = c("Publication Title", "Number of Documents in Corpus"), align = "cc", caption = "Table 2. Top 10 Journals in represented in the corpus")
```

Dictionary based Quantitative Text Analysis Based on the work of Montobbio et al. 2022

Truncated words

Theory based Watanabe.

Results

## Discussion

## Conclusions

## References
