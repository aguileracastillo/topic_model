---
title: "The needle and the haystack: A literature review using Structural Topic Modeling in a Digital Government Corpus"
author: "Andres Aguilera Castillo"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  pdf_document: default
  html_document: default
bibliography: references.bib
---

```{r data, include = FALSE}
library(tidyverse)
library(tidytext)
library(readr)
library(dplyr)
library(here)
library(naniar)
library(knitr)
library(magrittr)
library(ggplot2)
library(quanteda.textplots)
# Import DGRL version 17.5 (2021/12/15) 16531 References
# https://faculty.washington.edu/jscholl/dgrl/index.php 
# Master RIS file 
mf_ris <- read.csv(here("data", "DGRL_Lit_Master_v17_ris.csv"))
mf_ris <- as_tibble(mf_ris)
mf_ris <- mf_ris %>%
  replace_with_na_all(condition = ~.x %in% common_na_strings)

## Rename Variables of Interest
mf_ris <- mf_ris %>% rename(type = `Item.Type`, year = `Publication.Year`, author = `Author`, doc_title = `Title`, pub_title = `Publication.Title`, abstract = `Abstract.Note`)

# Corpus after initial cleaning
to_corpus <- read.csv(here("data", "to_corpus.csv"))

```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Abstract

Digital Government is a growing and vibrant multidisciplinary field of research, the fast increase in research output has challenged researchers to explore and use novel computational ways and methods to perform evidence synthesis on the extant literature and be able to map a scientific discipline, explore the thematic evolution over time and identify potential avenues for further research. Topic modeling has emerged as a powerful technique from the computer science field that is contributing to the examination of large amounts of text data. This manuscript demonstrates the training of a topic model aimed to assemble a 'smart literature review' on a subset of the Digital Government Reference Library (DGRL) version 17.5. Structural topic modeling is a conceptual and methodological evolution of 'vanilla' topic modeling that allow the estimation of covariates contained in the metadata of corpora to calculate topic prevalence in a corpus. To our best knowledge, this is the first attempt to use unsupervised machine learning techniques with this data set. This effort may contribute to creating a map of the field, identify the evolving themes in the literature and help to identify promising areas of research.

Findings?

## Introduction

Recent trends in global scientific output demonstrate a rapid and sustained increase in the production of vast amounts of unstructured data in the form of digitized text. This bounty in content is challenging researchers to explore and pursue novel methodological approaches and techniques to examine massive volumes of scientific publications in a systematic, efficient and reproducible manner. The expanding amount of bibliographic information available is exceeding traditional approaches for processing research output making it necessary to apply computational-assisted approaches for science mapping and evidence synthesis.

Topic modeling is an iterative process, thus this manuscript explores the training set[^1] of abstracts of journal articles contained in the Digital Government Research Library (DGRL) via a Structural Topic Model. Probabilistic topic models are a type of unsupervised machine learning processes that allow the exploration of a vast collection of documents (also known as corpus), perform the automated classification of large amounts of textual data and hence assist scholars in research tasks such as discovery, measurement, prediction and causal inference. Topic modeling enables the use of larger bibliographic data sets, and the extraction of relevant concepts from sizable corpora in a scalable way. To our best knowledge this is the first attempt to run a topic model for a corpus in the field of Digital Government Research.

[^1]: Best practices found in the literature suggest the split of sample data for topic modeling between a training and test data set. The structural topic model used in this manuscript have been trained on 75% of the data. A subsequent product will make use of the held out data.

The corpus used for this analysis is a subset of the journal articles in the version 17.5 of the Digital Government Reference Library. As argued by @grimmer2022a, texts are "expensive to produce, gather and collate", the contents of previous versions of this data set have been used as primary or secondary source of data exploring the Digital Government field. The Digital Government Research Library is a collection of bibliographic references associated with Digital Government scholarship. In its 17.5 version, it contains more than 16500 references, including journal articles, book chapters and conference papers. Contributions to this research domain come from established disciplines such as information science, computer science, organization science, sociology, public administration, and political science [@scholl2021].

Previous explorations of this reference library have revealed the thematic evolution using bibliometric and scientometric approaches [@alcaide-muñoz2017], and identified the most influential journals, conferences and leading scholars in the field [@scholl2021a]. This data set has been used as well for conducting a systematic review on the impacts of e-Government using a public value perspective [@maclean2021a]. According to [@webster2002], accumulating a "complete census" of the relevant literature and following a concept-centric framework are crucial in a literature review. Concept-centric approaches with topic modeling might be conducted via the use of "seed word dictionaries" in semi-supervised topic models [@watanabe2020], but this technique is out of the scope of this manuscript.

The study of the linkage between modern technologies and the quality and quantity labor has been on the research agenda of diverse disciplines and academic fields such as economics[@dosi2021; @fernández-macías2022], industrial relations [@doellgast2022a], information systems [@klein2021], and organization studies [@barley2020a], primarily focused in the private sector from advanced democracies. Technological change is a very broad term that may include a wide array of ICT-enabled applications for automation, digitalization and robotization. Our attention is directed at the digitalization of government, but despite the momentum in digital government research, one aspect that remains under explored is the empirical assessment of the effects of digital technologies on the public sector workforce [@plesner2018].

The public sector "composition" can be very heterogeneous in terms of scale and scope among diverse jurisdictions. Public sector organizations rank high globally among the largest employers in the form of armies and other defense related operations, State-Owned Enterprises (SOE), and health care providers, to name a few. The 'industries' in which public organizations operate are very diversified, have distinct degrees of technological sophistication and mixed levels of interaction with citizens and firms. In general, the public sector commands a large, diverse, and highly educated workforce.

The public sector is also considered one of the largest adopters and users of ICT, and perform a key role in the creation and governance of enormous amounts of data [@guenduez2020; @lofgren2020]. Historically, governments have developed the required information infrastructure to manage data intensive operations such as population and property registries, tax collection, and medical records among others. The pervasive deployment and use of digital technologies, digital platforms, and digital infrastructures has accelerated the rate of new data creation thus transforming the operations of firms and public organizations with profound implications for the organization of work [@nambisan2019].

The reported impact of digitalization on the organization work is diverse [@barley2001], it may automate work, create or eliminate jobs, deskill or reskill workers but also, little or negligible impact whatsoever. Digital government as a research field is in a phase of consolidation, allowing for the exploration of promising subfields for further inquiry. Digital technologies and the novel design of public services may facilitate a more intricate division of labor into smaller components (tasks), reconfiguring the workflow of public services, fostering new ways for multi-actor co-production [@bryson2016a], promoting the implementation of self-service solutions and facilitating scenarios for the co-production of public services [@scupola2021], turning each citizen and user into "his or her own administrator, caseworker and bureaucrat" [@schou2018], and possibly creating detrimental effects such as administrative burden for citizens [@madsen2021].

These developments enabled by the implementation of digital technologies in public organizations are changing the interaction between citizens and public officials turning it into a technology-mediated public encounter [@lindgren2019], introducing changes in the organization of work in terms of task redundancies and the creation of new occupations, to cope with an increasingly digitalized public sector. The argumentative arc presented above sparks the discussion of automation in a public sector context as an emerging topic of interest in the extant literature [@engin2019b; @andersson2021b; @lloyd2021a].

Conceptual developments in Digital Government Research have considered the success factors of digital government initiatives from both the supply and demand sides. However, given the intrinsic complexity associated with the public sector, a more elaborate discussion is found in the design and use literature that incorporates analytic dimensions such as power, ideology, design, and institutional change in the study of how novel technologies affect the organization of work [@bailey2020].

The relationship between digitalization and work is complex and multifaceted, its impacts are variegated among organizations, industries and employee groups [@doellgast2022b], however, it was the global health emergency in early 2020 that favored the newly gained awareness and increased the research interest in the subject matter [@nagel2020; @dingel2020; @mazzucato2020; @leonardi2021; @faraj2021]. Thus, it is deemed pertinent and timely to pursue the scholarly exploration of the effects of technological change in public organizations and its consequences for the public sector workforce.

The use of text based techniques and topic models have gained traction among scholars exploring the nexus between novel technologies and labor markets. Among these novel approaches are [@montobbio2022] that explore robots and labor-saving technologies, and [@kogan2019] that analyze patent contents to estimate technological change and labor displacement.

Supervised, semi-supervised and unsupervised machine learning techniques for text analysis can be used in a wide range of disciplines to examine databases, repositories and corpora, hence expanding the methodological repertoire of researchers opening an opportunity to explore large troves of data. It is our opinion that this methodological innovation can be repurposed to explore the linkages between digitalization and organization of work in a public sector context.

This initial argumentation lead us to formulate the following research questions:

RQ1: What does topic modeling techniques applied to the Digital Government Research Library v17.5 reveal about the conceptual, intellectual and thematic evolution this academic field? --\> Text mining and STM

RQ2: What structural changes can be interpreted from the topic model? --\> (Covariates)

RQ3: What does the extant literature (corpus) on Digital Government reveal on the linkage between digitalization and the organization of work? -- SeededLDA? (in-progress)

The objective of this exercise is to analyze and present the results of the application of a structural topic model, a novel method for evidence synthesis, in the exploration of the effects of digitalization in the organization of work in the public sector. The advent of computerization and digitalization has had broad impacts in most aspects of contemporary life, including scientific research. Digitalization has influenced how research is designed and conducted, allowing for the creation and increased availability of ever-growing data sets that require powerful computational methods and enhanced tools to handle abundant information [@meyer2015]. Therefore, this manuscript aims to offset the reported "excessive use" of qualitative methods in e-government research [@arduini2014a], and answering to calls in the extant literature towards the pursuit of quantitative and empirically oriented approaches [@wirtz2016].

## Literature Review and Conceptual Framework

Studies in the history of science have identified a relatively sustained growth pattern in scientific publications over time, this exponential growth rate means a doubling in scientific output every 17 years approximately [@bornmann2021]. This level of growth might be attributed to the increased resources dedicated to the global scientific endeavor and consequently the communication of science via publications. However, it may also be due to what has been dubbed "salami sliced publishing" or the multiple publications of a single research study [@bornmann2007a; @bornmann2015].

Research synthesis is part of the literature review process in which the extant scientific knowledge in each academic field is examined to help scholars understand the conceptual structure, themes, and debates to identify trends in the literature and potential areas for further research. This crucial task is labor-intensive, time-consuming, and restricted to a limited number of documents if conducted by traditional "manual" methods [@antons2020a; @asmussen2019]. Still, computer-assisted text analysis does not substitute human intervention, instead it "augments our reading ability" [@grimmer2022], human judgement is deemed necessary for the evaluation and validation of the outcome of these models [@barberá2021].

Quantitative research synthesis techniques like bibliometrics and computer-assisted text mining allow the analysis of a larger quantity of documents and may contribute to advancing the "research fronts" in interdisciplinary fields such as Digital Government [@tanskanen2017]. Computational tools and techniques developed originally in the computer science field have been repurposed in diverse disciplines but also have enabled social scientists to exploit Natural Language Processing (NLP) applications for classification tasks of large scientific corpora. Topic modeling techniques, a subset of machine learning and NLP allow for the automatic classification of vast amounts of text data.

Unstructured text has become one of the most prevalent types of data in the current "data deluge". In organization research, text is considered a key source of data as organizations publish content on their websites, social media and other searchable repositories [@kobayashi2017]. The use of text analysis or text mining is not necessarily new; however, the digitalization of everyday life has facilitated the creation, storage and analysis of enormous quantities of data in text format. Nonetheless the usage of text mining techniques has remained "disconnected among fields" [@banks2018].

Probabilistic topic modeling is a method that extracts topics from a collection of text. According to the seminal work by [@blei2003b], Latent Dirichlet Allocation (LDA) applied to a corpus generates a probabilistic model in which documents are represented as the mixtures of latent topics, and topics are characterized by a distribution of words. LDA is considered the state-of-the-art, simplest and most used method to perform topic modeling [@asmussen2019a].

LDA models are becoming widely used in social science, however these techniques are not infallible and require rigorous validation and human interpretability [@maier2018], if not, it may be as factual as "reading tea leaves" as eloquently put it by [@chang2009]. For a robust analysis it is advised to take an iterative approach for build, compute, critique, and rebuild topic models [@blei2014].

Even though these techniques originated in the computer science field and at first sight may seem arcane to newcomers, there have been important progress in other research areas towards facilitating the adoption of this powerful computational tool by lowering the technical barriers, the creation of agreed-upon workflows for modeling and visualization, and the development of relatively accessible software packages in open source statistical software like R and Python [@rehurek2010; @benoit2018a; @roberts2019].

Topic modeling techniques applied to bibliographic data have been explored in diverse scientific realms and academic disciplines such as statistics [@debattisti2015], economics [@ambrosino2018], cliometrics [@wehrheim2019], innovation research [@antons2020b; @antons2017], and management [@hannigan2019]. The scope of these analyses can be very large, [@ambrosino2018] studied the evolution in the thematic structure of the economics discipline by applying LDA to the full texts of articles published in 188 journals in the JSTOR database from 1845 to 2013 (n= 250846). Other implementations have concentrated its attention and analysis, [@antons2016] explored the full text corpus (n=1008) of a single top ranking journal in innovation research over a three decade span.

Structural Topic Modeling is a conceptual and technical evolution of the the typical topic modeling approach by incorporating the estimation of topic prevalence using covariates found in the metadata of the corpus [@roberts2016]. Applications of this method to bibliographic data have estimated the role of covariates such as temporal and geographic information in the analysis of the dissertation titles in economics and chemistry in East and West Germany before and after the German reunification [@rehs2020].

As advised by [@barberá2021], there are "consequential decisions" in the methodological choices of automated text classification and the fact that human validation is a key component of text as data methods. The selection of a corpus in itself is deemed a crucial decision that can be prone to four types of bias: resource bias, incentive bias, medium bias and retrieval bias, these selection biases are well acknowledged in the text as data literature [@grimmer2022]. It may be probable that the DGRL v17.5 has omitted important research that is not included in this collection. All decisions concerning text as data methodologies are "consequential", our aim is to make our workflow reproducible by documenting all the choices in the scripts associated with this document.

LDA is an unsupervised machine learning method which means the relationship between words and topics is ignored prior to the execution of the model. Thus is deemed good practice to split the data between a training set and a test set. Our approach is to train the model with 75% of the corpus, leaving the remaining proportion for testing purposes. The optimal number of topics (k) is unknown and the researcher should selected this parameter, there is technically no "right number of topics" and this choice might be specific to a corpus and research design [@grimmer2013]. In general, a low number of topics is used for an overview, instead, a higher number of topics is used for more granular analysis of the corpus [@asmussen2019a].

The evaluation of topic models can be performed through the calculation of goodness of fit statistics and the iterative calibration of the model to increase interpretability via "eye balling" the topics and their word-probability, and human judgement, meaning the implicit knowledge of the researcher on the subject matter of a corpus. A rule of thumb found in the documentation of the stm R package states that for small corpora, like the one used for this analysis containing "a few hundred to a few thousand" documents, 5 to 50 topics is "a good place to start, then an iterative calibration of the model is due. In addition, the stm R package includes functions for model selection, visualization and estimation of the effects of covariates in topic prevalence [@roberts2019a].

Four goodness of fit measures are usually considered when exploring the optimal number of topics to apply to a corpus: perplexity, coherence, residuals and lower bound. The held-out likelihood, also know as perplexity, measures how well the probability model predicts unseen data, a lower number in this measure implies a higher the accuracy of the model. Semantic coherence is maximized when the most probable words in a topic co-occur frequently [@roberts2014]. The lower bound indicator explains the convergence in the iterations of the model, when there is small change among iterations the model is considered converged. As for residuals, this diagnostic measure calculates the sample dispersion, if the number for this value is greater than one (\>1) it suggests that the number of topics are set too low [@taddy2011].

Text is a type of unstructured data that requires intensive processing to be able to work with it. This means that before being able to create and analyze a corpus object containing the information deemed of interest, "consequential decisions" have to be made. It is considered a best practice to use version control systems in the all the phases of the analysis for efficiency but also for replicability and transparency purposes.

Text data is incredibly diverse in length and contents. Social media posts, political speeches, press releases and customer reviews are the usual targets of this kind analysis. For researchers exploring bibliographic data the unit of analysis can be the title of the document, the abstract or the whole text of the documents in the corpus. Text data can be coerced into a type of structure for processing using the bag of words approach. The bag of words assumption means that the order of words within each document is ignored and the thematic structure of the document can be inferred by the frequency distribution of words [@maier2018a].

The bag of words approach deliberately ignores the syntax or structure of the text, the creation of a bag of words is known as tokenizing. Additional treatment of text include the elimination of punctuation, transform each word to lowercase and in some cases stemming which is a way to reduce a word to its stem or root in order to reduce the sparsity of the resulting matrices. Even though these steps may seem difficult to understand at first, the publication of open software packages, the availability of vast documentation, tutorials and vibrant online knowledge communities have lowered the technical barriers of this powerful computational tool for research.

The next step in pre-processing is the creation of the document-feature matrix[^2] containing all the documents and the tokenized text, the usual result is a very sparse matrix. Best practices found in the literature suggest to perform dimensionality reduction to the matrix by dropping features with very low frequency of occurrence and the very common features, the most common words in the corpus given that it is assumed that these very common words will not contribute to the discovery of the latent structure of the corpus.

[^2]: In the quanteda R package the Document-Feature Matrix is equivalent to the Document-Term Matrix of alternative text mining software. Features in this context are the individual tokens (single words) from the documents in the corpus.

Computational tools like topic models are enabling researchers to explore and analyze larger data sets of bibliographic information to conduct evidence synthesis by facilitating the exploration of a vast corpora, perform the automated classification of textual data and assist scholars in research tasks such as discovery, measurement, prediction and causal inference.

## Methods and Data

The Digital Government Research Library version 17.5 is a large curated repository of publications contributing to the field of Digital Government Research (DGR), it contains more than 16500 references among its records. The most prevalent types of documents are conference papers (33.2%) and journal articles (50%). The inclusion criteria of the DGRL are: to have passed academic peer review, to be published in an academic journal, to be published in English language [@scholl2021]. The Library can be downloaded from the website [DGRL](https://faculty.washington.edu/jscholl/dgrl/index.php).

```{r types of documents, echo=FALSE}
by_type <- mf_ris %>% count(`type`) %>% arrange(desc(n))
kable(by_type, col.names = c("Document Type", "Number of Documents"), align = "cc", caption = "Contents of DGRL v17.5 without processing")
```

The download package contains three types of bibliographic files BibTeX, RIS, and ENL (EndNote). In its raw and unprocessed form, the data has a large proportion of missing values, mostly clustered in metadata not considered relevant for the analysis. By exploring the different bibliographic formats, BiBTeX, RIS and ENL files, we noticed that the data sets had a large amount of missing data and that some information was available in a file type but not other. The script for initial data wrangling and data transformation documents the steps and choices made to the initial filtering and de-duplication. The unique digital object identifier (DOI) served as a exact key to merge the data sets, also as a "quality control" step to retain documents with valid DOIs.

The following variables have been deemed of interest for the analysis: type of reference (conference paper or journal article), year of publication, author, document title, publication title and the presence of an abstract. Text is a type of unstructured data that requires meticulous processing before using it. For replicability purposes, the script for the initial data cleaning and wrangling, including the R functions and packages used is available for revision, clarity, and replicability purposes and made publicly available in the scripts section of the [GitHub](https://github.com/aguileracastillo/topic_model) repository for this project.

After the initial data wrangling, the relevant data for 6682 journal articles or approximately 80.7% of the total number of articles in the DGRL v17.5 is further processed to create a corpus, the initial step towards a topic model. Then, documents published before year 2000 were dropped from further analysis due to their negligible quantity, also a single observation from year 2022. A visualization in the publication trend demonstrate an incipient increase in number of journal articles after year 2000 and a steep increase in the beginning of the 2010 decade to present.

```{r articles by year, echo = FALSE, include = TRUE, fig.align = "center", out.width= "75%"}
## Insert from 02_topic_model script
dgrl175_tm <- to_corpus %>% filter(type.x == "journalArticle")
dgrl175_tm <- dgrl175_tm %>% filter(year.x > 1999)
dgrl175_tm <- dgrl175_tm %>% filter(year.x < 2022)
dgrl175_tm %>% group_by(year.x) %>% count(sort = TRUE) %>%
  ggplot(aes(year.x, n)) +
  geom_line()+
  xlab ("Year") +
  ylab ("Number of Documents")
```

The subsequent step is the creation of a corpus object. A second script describes the phase of pre-processing related to preparing the unstructured text data into a format that is usable for analysis. Steps like tokenization, removal of stopwords, symbols, and special characters, and conversion to lowercase, are part of this phase [@maier2018a]. There are several software packages for text analysis, mining, and visualization, our choice for pre-processing was conducted in R statistical software using the functions of the quanteda R package [@benoit2018]. After several iterations, we deemed pertinent the use of an stemming algorithm to aid to the dimensionality reduction in the matrices by cutting words to their root form.

As suggested by @webster2002a, a complete review covers the relevant literature and it is not limited by a single research methodology, set of journals or geographic region, topic models contribute to expand the options available to researchers and the amplify the scope and reach of their inquiries. In this exercise, the top 10 publication titles (journal name) in the corpus represent almost a third of the documents in the sample. By making quick search in the Scimago Journal Rank website, it can be established that all publication titles on the table are listed in this database.

```{r top 10 journals in the sample, echo = FALSE, include = TRUE}
top10 <- dgrl175_tm %>% count(pub_title.x) %>% arrange(desc(n)) %>% head(10)
kable(top10, col.names = c("Publication Title", "Number of Documents in Corpus"), align = "cc", caption = "Top 10 Journals in represented in the corpus")
```

Text as data methods are inherently iterative thus requiring the adoption of suitable workflows and best practices for model calibration and version control systems of its operations, even though stop words are considered language specific and Natural Language Processing applications are advancing in sophistication, some stop words are corpus specific. Case in point, in the downstream of this process we found a strings with no meaning at all and a cluster of words that were very prevalent in the corpus but added no value to the analysis including "elsevier", "ltd\*", "all", "rights", "reserved", "abstract", "copyright\*", "inc\*", "e.g\*".

Quanteda pre-processing workflow includes functions that allow the finding of multi-word expressions via collocation analysis, this might be useful to identify proper names and other meaningful words. However, the "killer application" is the finding of n-grams, meaning the identification of single words (unigrams), or sequence of words (bigrams, trigrams) that tend to occur together with high frequency and may carry valuable information about the contents of the corpus.

```{r , echo=FALSE, out.width="100%", warning=FALSE, fig.cap ="DGRL Corpus Bigrams Visualization"}


knitr::include_graphics(here("graphics", "bigram_dgrl175.png"))
```

This visualization shows the top 40 bigrams found in the corpus, it can be seen that the words have been stemmed to their root form, the most salient bigram is "social_media" implying the centrality of these platforms for digital government scholarship, from adoption and use by public organizations [@mergel2013; @mergel2013], to the role of social media in political campaigns [@karlsen2010; @mascheroni2013], the regulation of disinformation [@marsden2020], and the provision of public services [@tursunbayeva2017; @criado2021].

Other salient bigrams include "smart_citi", "local_govern", and "open_data". A closer examination provides hints on methodological aspects, the bigram "case_studi" carries a lot of meaning informing about the frequency of this method in the sample. The ubiquity of the bigram "public_valu", for the public value theory shows the important evolution from New Public Management to alternative theoretical frameworks [@panagiotopoulos2019]. For a summary of the most frequent theoretical frameworks used in Digital Government Research refer to the work of @bannister2015a.

The ubiquity of the word service in bigrams such as "public_servic", "servic_deliveri", "servic_qualiti", "govern_servic" that provide a glimpse in the nature of government operations, the creation of public services but not necessarily with a service logic as argued by @cordella2018. This also suggests the influence of the work by @vargo2004a on service-dominant logic and its conceptual evolution including digital aspects [@barrett2015], and the adaptation to a public sector context by introducing a "public service logic" [@osborne2017], evidencing the rich conceptual roots from the field of service innovation studies that support digital government scholarship.

```{r , echo=FALSE, out.width="100%", warning=FALSE, fig.cap ="DGRL Corpus Trigrams Visualization"}

knitr::include_graphics(here("graphics", "trigram_dgrl175.png"))
```

This visualization shows the top 30 trigrams found in the corpus, clearly dominant in the wordcloud is "open_govern_data", this is a concept usually associated with the public value theory. A literature review on the public value of e-government found that open government data contributes to values like openness, transparency, participation, communication and collaboration [@twizeyimana2019].

The next salient trigram is "structur_equat_model", the dominant position of this trigram in the visualization of this contrasts the reported over-reliance of qualitative methods in the digital government field, also the trigram "partial_least_squar" hint of the importance of these methodologies in the corpus. Definitely something worth exploring deeper.

The trigram "technolog_accept_model" refers to the technology acceptance model found in the seminal work of [@davis1989] and adapted to the digital government field by [@hung2006], also the trigram "new_public_manag" provides a glimpse of the prevalence of this "paradigm" in the corpus [@oflynn2007].

The visualizations above are steps conducted before the casting of a Document-Feature Matrix, which is the method to be able to conduct the quantitative analysis of the corpus. From this step in the process we gather that the corpus under study contains 6664 documents (abstracts) with their respective metadata and 18749 features (unique tokenized words). This is a very sparse matrix and the logical step is to conduct two process for dimensionality reduction: remove very common and very rare words. For the removal of rare words, the parameter was set to retain words with a minimum term frequency of 100, for the most common words, the criteria was to remove words that appears in more than 10% of the documents in the corpus. After these decisions the number of documents remain the same, the number of features, the vocabulary that will be used for the topic model, was reduced to 916. The script for these steps is documented in the GitHub page for this project for replicability purposes.

The next steps in the preparation of the data to for topic modeling is to split the data between a train and a test set, at a proportion of 75/25 respectively. The structural topic model presented below has been calibrated with the the training set, the next logical step is to use the calibrated parameters with the unseen data. As before, the novelty of the structural topic model is possibility to include covariates found in the metadata to estimate topic prevalence, for our analysis, it was used the year of publication as covariate of interest.

The stm package includes the function searchK() that performs the estimation models with different K values to provide statistical analysis for goodness of fit measures in topic modeling. Perplexity, semantic coherence, residuals and lower bound can be visualized as a result of this function thus helping researchers to select the optimal number of topics. However, statistical goodness of fit is not enough and it is widely advised to apply human validation and human judgement in the decision of the number of topics to model.

The following graphic show the diverse goodness of fit measures calculated for different values for K in a range from 25 to 60 topics that can be used for hints for further exploration and analysis, the values for held-out likelihood, or perplexity seem to be optimal for x values, semantic coherence is higher in topics X, Y and Z, the residuals values above 1 indicate sample dispersion that is interpreted as the number of K is too low, the lower bound value indicate model convergence, small changes between the compared values are preferred.

## Discussion

The identification and visualization of bigrams and trigrams enrich the researcher's ability to have a quick overview of the co-ocurrence of words, this contribute to the quick detection of meaning word combinations contributing to the interpretation of the contents of the corpus.

Key term extractions from the corpus - Stemming

Collect names (Buffat et al)

Insert bigrams and trigrams graphics

## Conclusions

## References
